{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e932d77c",
   "metadata": {},
   "source": [
    "1,What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "\n",
    "Feature engineering is the process of selecting, transforming, and creating features (i.e., variables) from the raw data that can improve the performance of a machine learning model. The goal of feature engineering is to create features that are relevant, informative, and not redundant, to help the model make accurate predictions.\n",
    "\n",
    "Aspects of feature engineering:\n",
    "\n",
    "Feature Selection: This involves selecting only the most important features from the data to reduce the dimensionality and complexity of the model.\n",
    "\n",
    "Feature Transformation: This involves transforming the features to a different representation, such as scaling, normalization, or binarization, to improve the model's performance.\n",
    "\n",
    "Feature Creation: This involves creating new features from the existing ones by combining, deriving, or extracting them, such as calculating age from date of birth, or extracting text features from unstructured data.\n",
    "\n",
    "Feature Scaling: This involves scaling the features to a specific range to ensure that they have the same impact on the model.\n",
    "\n",
    "Python code example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f0d02e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 4\n",
      "Selected features: 2\n"
     ]
    }
   ],
   "source": [
    "#Feature selection using SelectKBest\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "#Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "#Select the top 2 features using ANOVA F-test\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(\"Original features:\", X.shape[1])\n",
    "print(\"Selected features:\", X_new.shape[1]) # Output: Selected features: 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b6cec",
   "metadata": {},
   "source": [
    "Q2: What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "\n",
    "Feature selection is the process of selecting a subset of the most relevant features from the original set of features. The aim of feature selection is to reduce the dimensionality of the data, eliminate irrelevant and redundant features, and improve the model's performance.\n",
    "\n",
    "Methods of feature selection:\n",
    "\n",
    "Filter methods: This method involves selecting features based on their statistical significance and correlation with the target variable, such as ANOVA F-test, Chi-squared test, and correlation matrix.\n",
    "\n",
    "Wrapper methods: This method involves selecting features based on their predictive power using a specific machine learning model, such as Recursive Feature Elimination and Forward Selection.\n",
    "\n",
    "Embedded methods: This method involves selecting features during the model training process, such as Lasso and Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "247fd506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: [2 3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# perform feature selection using ANOVA F-value\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# print the indices of the selected features\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "print('Selected features:', selected_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac00521",
   "metadata": {},
   "source": [
    "3.Describe the feature selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "In feature selection, the filter and wrapper approaches are the two main methods used to select the features for the model.\n",
    "\n",
    "The filter approach uses statistical measures to assign a score to each feature. The score reflects the relevance of the feature to the target variable. Then, the features with the highest scores are selected for the model. This approach is simple and computationally efficient. However, it does not take into account the interaction between the features and their impact on the model's performance.\n",
    "\n",
    "On the other hand, the wrapper approach involves selecting features based on the model's performance. It works by selecting a subset of features, training the model on that subset, and evaluating its performance. The process is repeated, and different subsets of features are selected each time. This approach can take into account the interaction between the features, leading to better model performance. However, it can be computationally expensive, especially for large datasets.\n",
    "\n",
    "In summary, the filter approach is a quick and easy way to select relevant features, but it may not always result in the best performance. The wrapper approach, although more computationally expensive, can lead to better model performance by considering feature interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30146a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n",
      "[[1.4 0.2]\n",
      " [1.4 0.2]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.7 0.4]\n",
      " [1.4 0.3]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.1]\n",
      " [1.5 0.2]\n",
      " [1.6 0.2]\n",
      " [1.4 0.1]\n",
      " [1.1 0.1]\n",
      " [1.2 0.2]\n",
      " [1.5 0.4]\n",
      " [1.3 0.4]\n",
      " [1.4 0.3]\n",
      " [1.7 0.3]\n",
      " [1.5 0.3]\n",
      " [1.7 0.2]\n",
      " [1.5 0.4]\n",
      " [1.  0.2]\n",
      " [1.7 0.5]\n",
      " [1.9 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.4]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [1.6 0.2]\n",
      " [1.6 0.2]\n",
      " [1.5 0.4]\n",
      " [1.5 0.1]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.2 0.2]\n",
      " [1.3 0.2]\n",
      " [1.4 0.1]\n",
      " [1.3 0.2]\n",
      " [1.5 0.2]\n",
      " [1.3 0.3]\n",
      " [1.3 0.3]\n",
      " [1.3 0.2]\n",
      " [1.6 0.6]\n",
      " [1.9 0.4]\n",
      " [1.4 0.3]\n",
      " [1.6 0.2]\n",
      " [1.4 0.2]\n",
      " [1.5 0.2]\n",
      " [1.4 0.2]\n",
      " [4.7 1.4]\n",
      " [4.5 1.5]\n",
      " [4.9 1.5]\n",
      " [4.  1.3]\n",
      " [4.6 1.5]\n",
      " [4.5 1.3]\n",
      " [4.7 1.6]\n",
      " [3.3 1. ]\n",
      " [4.6 1.3]\n",
      " [3.9 1.4]\n",
      " [3.5 1. ]\n",
      " [4.2 1.5]\n",
      " [4.  1. ]\n",
      " [4.7 1.4]\n",
      " [3.6 1.3]\n",
      " [4.4 1.4]\n",
      " [4.5 1.5]\n",
      " [4.1 1. ]\n",
      " [4.5 1.5]\n",
      " [3.9 1.1]\n",
      " [4.8 1.8]\n",
      " [4.  1.3]\n",
      " [4.9 1.5]\n",
      " [4.7 1.2]\n",
      " [4.3 1.3]\n",
      " [4.4 1.4]\n",
      " [4.8 1.4]\n",
      " [5.  1.7]\n",
      " [4.5 1.5]\n",
      " [3.5 1. ]\n",
      " [3.8 1.1]\n",
      " [3.7 1. ]\n",
      " [3.9 1.2]\n",
      " [5.1 1.6]\n",
      " [4.5 1.5]\n",
      " [4.5 1.6]\n",
      " [4.7 1.5]\n",
      " [4.4 1.3]\n",
      " [4.1 1.3]\n",
      " [4.  1.3]\n",
      " [4.4 1.2]\n",
      " [4.6 1.4]\n",
      " [4.  1.2]\n",
      " [3.3 1. ]\n",
      " [4.2 1.3]\n",
      " [4.2 1.2]\n",
      " [4.2 1.3]\n",
      " [4.3 1.3]\n",
      " [3.  1.1]\n",
      " [4.1 1.3]\n",
      " [6.  2.5]\n",
      " [5.1 1.9]\n",
      " [5.9 2.1]\n",
      " [5.6 1.8]\n",
      " [5.8 2.2]\n",
      " [6.6 2.1]\n",
      " [4.5 1.7]\n",
      " [6.3 1.8]\n",
      " [5.8 1.8]\n",
      " [6.1 2.5]\n",
      " [5.1 2. ]\n",
      " [5.3 1.9]\n",
      " [5.5 2.1]\n",
      " [5.  2. ]\n",
      " [5.1 2.4]\n",
      " [5.3 2.3]\n",
      " [5.5 1.8]\n",
      " [6.7 2.2]\n",
      " [6.9 2.3]\n",
      " [5.  1.5]\n",
      " [5.7 2.3]\n",
      " [4.9 2. ]\n",
      " [6.7 2. ]\n",
      " [4.9 1.8]\n",
      " [5.7 2.1]\n",
      " [6.  1.8]\n",
      " [4.8 1.8]\n",
      " [4.9 1.8]\n",
      " [5.6 2.1]\n",
      " [5.8 1.6]\n",
      " [6.1 1.9]\n",
      " [6.4 2. ]\n",
      " [5.6 2.2]\n",
      " [5.1 1.5]\n",
      " [5.6 1.4]\n",
      " [6.1 2.3]\n",
      " [5.6 2.4]\n",
      " [5.5 1.8]\n",
      " [4.8 1.8]\n",
      " [5.4 2.1]\n",
      " [5.6 2.4]\n",
      " [5.1 2.3]\n",
      " [5.1 1.9]\n",
      " [5.9 2.3]\n",
      " [5.7 2.5]\n",
      " [5.2 2.3]\n",
      " [5.  1.9]\n",
      " [5.2 2. ]\n",
      " [5.4 2.3]\n",
      " [5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2, RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Filter method: Select the top 2 features using chi-squared test\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "print(X_new)\n",
    "\n",
    "# Wrapper method: Select the top 2 features using recursive feature elimination\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "print(X_new)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbb7f78",
   "metadata": {},
   "source": [
    "4\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee7711e",
   "metadata": {},
   "source": [
    "i. The overall feature selection process involves identifying and selecting a subset of the original features that are most relevant to the task at hand. This process typically involves two main steps:\n",
    "\n",
    "Feature representation: This step involves representing the data in a form that is suitable for analysis by a machine learning algorithm. For example, converting text data into numerical vectors or images into pixel values.\n",
    "\n",
    "Feature selection: This step involves selecting a subset of the features that are most important for the task at hand. This can be done using various methods, including filter methods, wrapper methods, and embedded methods.\n",
    "\n",
    "ii. The key underlying principle of feature extraction is to transform the original features into a new set of features that are more informative and more suitable for the machine learning algorithm being used. One of the most widely used feature extraction algorithms is Principal Component Analysis (PCA), which involves transforming the data into a new set of uncorrelated features called principal components. Other widely used feature extraction algorithms include Linear Discriminant Analysis (LDA) and t-distributed Stochastic Neighbor Embedding (t-SNE).\n",
    "\n",
    "Here's some sample Python code for performing PCA feature extraction using scikit-lear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aab7fa9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.68412563  0.31939725]\n",
      " [-2.71414169 -0.17700123]\n",
      " [-2.88899057 -0.14494943]\n",
      " [-2.74534286 -0.31829898]\n",
      " [-2.72871654  0.32675451]\n",
      " [-2.28085963  0.74133045]\n",
      " [-2.82053775 -0.08946138]\n",
      " [-2.62614497  0.16338496]\n",
      " [-2.88638273 -0.57831175]\n",
      " [-2.6727558  -0.11377425]\n",
      " [-2.50694709  0.6450689 ]\n",
      " [-2.61275523  0.01472994]\n",
      " [-2.78610927 -0.235112  ]\n",
      " [-3.22380374 -0.51139459]\n",
      " [-2.64475039  1.17876464]\n",
      " [-2.38603903  1.33806233]\n",
      " [-2.62352788  0.81067951]\n",
      " [-2.64829671  0.31184914]\n",
      " [-2.19982032  0.87283904]\n",
      " [-2.5879864   0.51356031]\n",
      " [-2.31025622  0.39134594]\n",
      " [-2.54370523  0.43299606]\n",
      " [-3.21593942  0.13346807]\n",
      " [-2.30273318  0.09870885]\n",
      " [-2.35575405 -0.03728186]\n",
      " [-2.50666891 -0.14601688]\n",
      " [-2.46882007  0.13095149]\n",
      " [-2.56231991  0.36771886]\n",
      " [-2.63953472  0.31203998]\n",
      " [-2.63198939 -0.19696122]\n",
      " [-2.58739848 -0.20431849]\n",
      " [-2.4099325   0.41092426]\n",
      " [-2.64886233  0.81336382]\n",
      " [-2.59873675  1.09314576]\n",
      " [-2.63692688 -0.12132235]\n",
      " [-2.86624165  0.06936447]\n",
      " [-2.62523805  0.59937002]\n",
      " [-2.80068412  0.26864374]\n",
      " [-2.98050204 -0.48795834]\n",
      " [-2.59000631  0.22904384]\n",
      " [-2.77010243  0.26352753]\n",
      " [-2.84936871 -0.94096057]\n",
      " [-2.99740655 -0.34192606]\n",
      " [-2.40561449  0.18887143]\n",
      " [-2.20948924  0.43666314]\n",
      " [-2.71445143 -0.2502082 ]\n",
      " [-2.53814826  0.50377114]\n",
      " [-2.83946217 -0.22794557]\n",
      " [-2.54308575  0.57941002]\n",
      " [-2.70335978  0.10770608]\n",
      " [ 1.28482569  0.68516047]\n",
      " [ 0.93248853  0.31833364]\n",
      " [ 1.46430232  0.50426282]\n",
      " [ 0.18331772 -0.82795901]\n",
      " [ 1.08810326  0.07459068]\n",
      " [ 0.64166908 -0.41824687]\n",
      " [ 1.09506066  0.28346827]\n",
      " [-0.74912267 -1.00489096]\n",
      " [ 1.04413183  0.2283619 ]\n",
      " [-0.0087454  -0.72308191]\n",
      " [-0.50784088 -1.26597119]\n",
      " [ 0.51169856 -0.10398124]\n",
      " [ 0.26497651 -0.55003646]\n",
      " [ 0.98493451 -0.12481785]\n",
      " [-0.17392537 -0.25485421]\n",
      " [ 0.92786078  0.46717949]\n",
      " [ 0.66028376 -0.35296967]\n",
      " [ 0.23610499 -0.33361077]\n",
      " [ 0.94473373 -0.54314555]\n",
      " [ 0.04522698 -0.58383438]\n",
      " [ 1.11628318 -0.08461685]\n",
      " [ 0.35788842 -0.06892503]\n",
      " [ 1.29818388 -0.32778731]\n",
      " [ 0.92172892 -0.18273779]\n",
      " [ 0.71485333  0.14905594]\n",
      " [ 0.90017437  0.32850447]\n",
      " [ 1.33202444  0.24444088]\n",
      " [ 1.55780216  0.26749545]\n",
      " [ 0.81329065 -0.1633503 ]\n",
      " [-0.30558378 -0.36826219]\n",
      " [-0.06812649 -0.70517213]\n",
      " [-0.18962247 -0.68028676]\n",
      " [ 0.13642871 -0.31403244]\n",
      " [ 1.38002644 -0.42095429]\n",
      " [ 0.58800644 -0.48428742]\n",
      " [ 0.80685831  0.19418231]\n",
      " [ 1.22069088  0.40761959]\n",
      " [ 0.81509524 -0.37203706]\n",
      " [ 0.24595768 -0.2685244 ]\n",
      " [ 0.16641322 -0.68192672]\n",
      " [ 0.46480029 -0.67071154]\n",
      " [ 0.8908152  -0.03446444]\n",
      " [ 0.23054802 -0.40438585]\n",
      " [-0.70453176 -1.01224823]\n",
      " [ 0.35698149 -0.50491009]\n",
      " [ 0.33193448 -0.21265468]\n",
      " [ 0.37621565 -0.29321893]\n",
      " [ 0.64257601  0.01773819]\n",
      " [-0.90646986 -0.75609337]\n",
      " [ 0.29900084 -0.34889781]\n",
      " [ 2.53119273 -0.00984911]\n",
      " [ 1.41523588 -0.57491635]\n",
      " [ 2.61667602  0.34390315]\n",
      " [ 1.97153105 -0.1797279 ]\n",
      " [ 2.35000592 -0.04026095]\n",
      " [ 3.39703874  0.55083667]\n",
      " [ 0.52123224 -1.19275873]\n",
      " [ 2.93258707  0.3555    ]\n",
      " [ 2.32122882 -0.2438315 ]\n",
      " [ 2.91675097  0.78279195]\n",
      " [ 1.66177415  0.24222841]\n",
      " [ 1.80340195 -0.21563762]\n",
      " [ 2.1655918   0.21627559]\n",
      " [ 1.34616358 -0.77681835]\n",
      " [ 1.58592822 -0.53964071]\n",
      " [ 1.90445637  0.11925069]\n",
      " [ 1.94968906  0.04194326]\n",
      " [ 3.48705536  1.17573933]\n",
      " [ 3.79564542  0.25732297]\n",
      " [ 1.30079171 -0.76114964]\n",
      " [ 2.42781791  0.37819601]\n",
      " [ 1.19900111 -0.60609153]\n",
      " [ 3.49992004  0.4606741 ]\n",
      " [ 1.38876613 -0.20439933]\n",
      " [ 2.2754305   0.33499061]\n",
      " [ 2.61409047  0.56090136]\n",
      " [ 1.25850816 -0.17970479]\n",
      " [ 1.29113206 -0.11666865]\n",
      " [ 2.12360872 -0.20972948]\n",
      " [ 2.38800302  0.4646398 ]\n",
      " [ 2.84167278  0.37526917]\n",
      " [ 3.23067366  1.37416509]\n",
      " [ 2.15943764 -0.21727758]\n",
      " [ 1.44416124 -0.14341341]\n",
      " [ 1.78129481 -0.49990168]\n",
      " [ 3.07649993  0.68808568]\n",
      " [ 2.14424331  0.1400642 ]\n",
      " [ 1.90509815  0.04930053]\n",
      " [ 1.16932634 -0.16499026]\n",
      " [ 2.10761114  0.37228787]\n",
      " [ 2.31415471  0.18365128]\n",
      " [ 1.9222678   0.40920347]\n",
      " [ 1.41523588 -0.57491635]\n",
      " [ 2.56301338  0.2778626 ]\n",
      " [ 2.41874618  0.3047982 ]\n",
      " [ 1.94410979  0.1875323 ]\n",
      " [ 1.52716661 -0.37531698]\n",
      " [ 1.76434572  0.07885885]\n",
      " [ 1.90094161  0.11662796]\n",
      " [ 1.39018886 -0.28266094]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Perform PCA to extract the top 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Print the resulting transformed features\n",
    "print(X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09615bb2",
   "metadata": {},
   "source": [
    "5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5022650e",
   "metadata": {},
   "source": [
    "Text categorization involves the process of assigning categories to text documents based on their content. The feature engineering process for text categorization can involve several steps, including:\n",
    "\n",
    "Text preprocessing: This involves removing noise, stop words, and other irrelevant information from the text data. It also involves stemming or lemmatization to reduce the words to their root forms.\n",
    "\n",
    "Feature extraction: This involves transforming the text data into a set of numerical features that can be used by machine learning algorithms. Common feature extraction techniques include bag-of-words, term frequency-inverse document frequency (TF-IDF), and word embeddings.\n",
    "\n",
    "Feature selection: This involves selecting a subset of the most relevant features for use in the machine learning model. This is often done to reduce the dimensionality of the data and improve the performance of the model.\n",
    "\n",
    "Model training: This involves training a machine learning model on the selected features and labels.\n",
    "\n",
    "Here's an example of a Python code snippet that demonstrates the feature engineering process for text categorization using the bag-of-words technique and a linear support vector machine (SVM) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f192aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Preprocess the text data\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "y = newsgroups.target\n",
    "\n",
    "# Feature selection: Select the top 1000 features using chi-squared test\n",
    "selector = SelectKBest(chi2, k=1000)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3)\n",
    "\n",
    "# Train a linear SVM model on the selected features\n",
    "model = LinearSVC()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "accuracy = model.score(X_test, y_test)\n",
    "print(f'Test accuracy: {accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a31d845",
   "metadata": {},
   "source": [
    "In this code snippet, we first load the 20 Newsgroups dataset and preprocess the text data using the CountVectorizer class from scikit-learn. We then perform feature selection using the SelectKBest class and the chi-squared test to select the top 1000 most informative features. Finally, we train a linear SVM model on the selected features and evaluate its performance on a testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ae11ce",
   "metadata": {},
   "source": [
    "6.What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd15eb3",
   "metadata": {},
   "source": [
    "Cosine similarity is a metric used to determine the similarity between two non-zero vectors of an inner product space. It is frequently used in text analysis to determine how similar two documents are based on their content. The cosine similarity measures the cosine of the angle between the two vectors, which is computed as the dot product of the vectors divided by the product of their magnitudes. It ranges between -1 and 1, where 1 indicates that the vectors are identical, 0 indicates that they are orthogonal, and -1 indicates that they are diametrically opposed.\n",
    "\n",
    "To calculate the cosine similarity between the two rows of a document-term matrix, we first need to compute the dot product of the two vectors, and then divide it by the product of their magnitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eba77afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.67530325]\n",
      " [0.67530325 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create the document-term matrix\n",
    "dtm = np.array([[2, 3, 2, 0, 2, 3, 3, 0, 1], [2, 1, 0, 0, 3, 2, 1, 3, 1]])\n",
    "\n",
    "# Compute the cosine similarity\n",
    "similarity = cosine_similarity(dtm)\n",
    "print(similarity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36e9960",
   "metadata": {},
   "source": [
    "cosine_similarity(x, y) = dot_product(x, y) / (magnitude(x) * magnitude(y))\n",
    "\n",
    "where dot_product(x, y) is the dot product of vectors x and y, and magnitude(x) and magnitude(y) are the magnitudes of vectors x and y, respectively.\n",
    "\n",
    "In the given example, the two vectors are (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Using the formula above, we can compute their cosine similarity as follows:\n",
    "\n",
    "dot_product = (22) + (31) + (20) + (00) + (23) + (32) + (31) + (03) + (1*1) = 25\n",
    "magnitude_x = sqrt(2^2 + 3^2 + 2^2 + 0^2 + 2^2 + 3^2 + 3^2 + 0^2 + 1^2) = sqrt(38)\n",
    "magnitude_y = sqrt(2^2 + 1^2 + 0^2 + 0^2 + 3^2 + 2^2 + 1^2 + 3^2 + 1^2) = sqrt(30)\n",
    "\n",
    "cosine_similarity = dot_product / (magnitude_x * magnitude_y) = 25 / (sqrt(38) * sqrt(30)) = 0.822"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc1852",
   "metadata": {},
   "source": [
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26401d6",
   "metadata": {},
   "source": [
    "i. The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. The formula for calculating Hamming distance is:\n",
    "\n",
    "d(x,y) = ∑ i=1^n (xi ≠ yi)\n",
    "\n",
    "where n is the length of the strings x and y.\n",
    "\n",
    "Using this formula, we can calculate the Hamming distance between the strings \"10001011\" and \"11001111\" as follows:\n",
    "\n",
    "d(\"10001011\",\"11001111\") = (1 ≠ 1) + (0 ≠ 1) + (0 ≠ 0) + (0 ≠ 0) + (1 ≠ 1) + (0 ≠ 1) + (1 ≠ 1) + (1 ≠ 1) = 4\n",
    "\n",
    "Therefore, the Hamming distance between \"10001011\" and \"11001111\" is 4.\n",
    "\n",
    "ii. The Jaccard index and similarity matching coefficient are both measures of similarity between sets. The Jaccard index is defined as the size of the intersection of two sets divided by the size of the union of the two sets:\n",
    "\n",
    "J(A,B) = |A ∩ B| / |A ∪ B|\n",
    "\n",
    "The similarity matching coefficient is defined as the size of the intersection of two sets divided by the size of the smaller of the two sets:\n",
    "\n",
    "SMC(A,B) = |A ∩ B| / min(|A|, |B|)\n",
    "\n",
    "Using these formulas, we can calculate the Jaccard index and similarity matching coefficient of the two features as follows:\n",
    "\n",
    "A = (1, 1, 0, 0, 1, 0, 1, 1)\n",
    "B = (1, 0, 0, 1, 1, 0, 0, 1)\n",
    "\n",
    "J(A,B) = |{1, 4, 5, 7}| / |{0, 1, 2, 3, 4, 5, 6, 7}| = 4/8 = 0.5\n",
    "SMC(A,B) = |{1, 4, 5, 7}| / min(|A|, |B|) = 4 / min(8, 8) = 0.5\n",
    "\n",
    "Therefore, the Jaccard index and similarity matching coefficient of the two features are both 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1e875",
   "metadata": {},
   "source": [
    "8.State what is meant by \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n",
    "Answer:\n",
    "\n",
    "In machine learning, high-dimensional data refers to data sets that have a large number of features or attributes. When the number of features is significantly larger than the number of observations, it is known as the \"curse of dimensionality.\" High-dimensional data is common in fields such as bioinformatics, image analysis, natural language processing, and many others.\n",
    "\n",
    "The primary difficulty in using machine learning algorithms on high-dimensional data sets is the increased computational complexity and the risk of overfitting. Overfitting occurs when a model is too complex and learns the noise in the data instead of the underlying pattern, resulting in poor generalization to new data. High-dimensional data can exacerbate this problem since the number of possible models increases exponentially with the number of features.\n",
    "\n",
    "Several techniques can be used to address the curse of dimensionality, including feature selection, feature extraction, and dimensionality reduction techniques such as principal component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE). These techniques aim to reduce the number of features while preserving the most important information in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8feba6d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.68412563,  0.31939725],\n",
       "       [-2.71414169, -0.17700123],\n",
       "       [-2.88899057, -0.14494943],\n",
       "       [-2.74534286, -0.31829898],\n",
       "       [-2.72871654,  0.32675451],\n",
       "       [-2.28085963,  0.74133045],\n",
       "       [-2.82053775, -0.08946138],\n",
       "       [-2.62614497,  0.16338496],\n",
       "       [-2.88638273, -0.57831175],\n",
       "       [-2.6727558 , -0.11377425],\n",
       "       [-2.50694709,  0.6450689 ],\n",
       "       [-2.61275523,  0.01472994],\n",
       "       [-2.78610927, -0.235112  ],\n",
       "       [-3.22380374, -0.51139459],\n",
       "       [-2.64475039,  1.17876464],\n",
       "       [-2.38603903,  1.33806233],\n",
       "       [-2.62352788,  0.81067951],\n",
       "       [-2.64829671,  0.31184914],\n",
       "       [-2.19982032,  0.87283904],\n",
       "       [-2.5879864 ,  0.51356031],\n",
       "       [-2.31025622,  0.39134594],\n",
       "       [-2.54370523,  0.43299606],\n",
       "       [-3.21593942,  0.13346807],\n",
       "       [-2.30273318,  0.09870885],\n",
       "       [-2.35575405, -0.03728186],\n",
       "       [-2.50666891, -0.14601688],\n",
       "       [-2.46882007,  0.13095149],\n",
       "       [-2.56231991,  0.36771886],\n",
       "       [-2.63953472,  0.31203998],\n",
       "       [-2.63198939, -0.19696122],\n",
       "       [-2.58739848, -0.20431849],\n",
       "       [-2.4099325 ,  0.41092426],\n",
       "       [-2.64886233,  0.81336382],\n",
       "       [-2.59873675,  1.09314576],\n",
       "       [-2.63692688, -0.12132235],\n",
       "       [-2.86624165,  0.06936447],\n",
       "       [-2.62523805,  0.59937002],\n",
       "       [-2.80068412,  0.26864374],\n",
       "       [-2.98050204, -0.48795834],\n",
       "       [-2.59000631,  0.22904384],\n",
       "       [-2.77010243,  0.26352753],\n",
       "       [-2.84936871, -0.94096057],\n",
       "       [-2.99740655, -0.34192606],\n",
       "       [-2.40561449,  0.18887143],\n",
       "       [-2.20948924,  0.43666314],\n",
       "       [-2.71445143, -0.2502082 ],\n",
       "       [-2.53814826,  0.50377114],\n",
       "       [-2.83946217, -0.22794557],\n",
       "       [-2.54308575,  0.57941002],\n",
       "       [-2.70335978,  0.10770608],\n",
       "       [ 1.28482569,  0.68516047],\n",
       "       [ 0.93248853,  0.31833364],\n",
       "       [ 1.46430232,  0.50426282],\n",
       "       [ 0.18331772, -0.82795901],\n",
       "       [ 1.08810326,  0.07459068],\n",
       "       [ 0.64166908, -0.41824687],\n",
       "       [ 1.09506066,  0.28346827],\n",
       "       [-0.74912267, -1.00489096],\n",
       "       [ 1.04413183,  0.2283619 ],\n",
       "       [-0.0087454 , -0.72308191],\n",
       "       [-0.50784088, -1.26597119],\n",
       "       [ 0.51169856, -0.10398124],\n",
       "       [ 0.26497651, -0.55003646],\n",
       "       [ 0.98493451, -0.12481785],\n",
       "       [-0.17392537, -0.25485421],\n",
       "       [ 0.92786078,  0.46717949],\n",
       "       [ 0.66028376, -0.35296967],\n",
       "       [ 0.23610499, -0.33361077],\n",
       "       [ 0.94473373, -0.54314555],\n",
       "       [ 0.04522698, -0.58383438],\n",
       "       [ 1.11628318, -0.08461685],\n",
       "       [ 0.35788842, -0.06892503],\n",
       "       [ 1.29818388, -0.32778731],\n",
       "       [ 0.92172892, -0.18273779],\n",
       "       [ 0.71485333,  0.14905594],\n",
       "       [ 0.90017437,  0.32850447],\n",
       "       [ 1.33202444,  0.24444088],\n",
       "       [ 1.55780216,  0.26749545],\n",
       "       [ 0.81329065, -0.1633503 ],\n",
       "       [-0.30558378, -0.36826219],\n",
       "       [-0.06812649, -0.70517213],\n",
       "       [-0.18962247, -0.68028676],\n",
       "       [ 0.13642871, -0.31403244],\n",
       "       [ 1.38002644, -0.42095429],\n",
       "       [ 0.58800644, -0.48428742],\n",
       "       [ 0.80685831,  0.19418231],\n",
       "       [ 1.22069088,  0.40761959],\n",
       "       [ 0.81509524, -0.37203706],\n",
       "       [ 0.24595768, -0.2685244 ],\n",
       "       [ 0.16641322, -0.68192672],\n",
       "       [ 0.46480029, -0.67071154],\n",
       "       [ 0.8908152 , -0.03446444],\n",
       "       [ 0.23054802, -0.40438585],\n",
       "       [-0.70453176, -1.01224823],\n",
       "       [ 0.35698149, -0.50491009],\n",
       "       [ 0.33193448, -0.21265468],\n",
       "       [ 0.37621565, -0.29321893],\n",
       "       [ 0.64257601,  0.01773819],\n",
       "       [-0.90646986, -0.75609337],\n",
       "       [ 0.29900084, -0.34889781],\n",
       "       [ 2.53119273, -0.00984911],\n",
       "       [ 1.41523588, -0.57491635],\n",
       "       [ 2.61667602,  0.34390315],\n",
       "       [ 1.97153105, -0.1797279 ],\n",
       "       [ 2.35000592, -0.04026095],\n",
       "       [ 3.39703874,  0.55083667],\n",
       "       [ 0.52123224, -1.19275873],\n",
       "       [ 2.93258707,  0.3555    ],\n",
       "       [ 2.32122882, -0.2438315 ],\n",
       "       [ 2.91675097,  0.78279195],\n",
       "       [ 1.66177415,  0.24222841],\n",
       "       [ 1.80340195, -0.21563762],\n",
       "       [ 2.1655918 ,  0.21627559],\n",
       "       [ 1.34616358, -0.77681835],\n",
       "       [ 1.58592822, -0.53964071],\n",
       "       [ 1.90445637,  0.11925069],\n",
       "       [ 1.94968906,  0.04194326],\n",
       "       [ 3.48705536,  1.17573933],\n",
       "       [ 3.79564542,  0.25732297],\n",
       "       [ 1.30079171, -0.76114964],\n",
       "       [ 2.42781791,  0.37819601],\n",
       "       [ 1.19900111, -0.60609153],\n",
       "       [ 3.49992004,  0.4606741 ],\n",
       "       [ 1.38876613, -0.20439933],\n",
       "       [ 2.2754305 ,  0.33499061],\n",
       "       [ 2.61409047,  0.56090136],\n",
       "       [ 1.25850816, -0.17970479],\n",
       "       [ 1.29113206, -0.11666865],\n",
       "       [ 2.12360872, -0.20972948],\n",
       "       [ 2.38800302,  0.4646398 ],\n",
       "       [ 2.84167278,  0.37526917],\n",
       "       [ 3.23067366,  1.37416509],\n",
       "       [ 2.15943764, -0.21727758],\n",
       "       [ 1.44416124, -0.14341341],\n",
       "       [ 1.78129481, -0.49990168],\n",
       "       [ 3.07649993,  0.68808568],\n",
       "       [ 2.14424331,  0.1400642 ],\n",
       "       [ 1.90509815,  0.04930053],\n",
       "       [ 1.16932634, -0.16499026],\n",
       "       [ 2.10761114,  0.37228787],\n",
       "       [ 2.31415471,  0.18365128],\n",
       "       [ 1.9222678 ,  0.40920347],\n",
       "       [ 1.41523588, -0.57491635],\n",
       "       [ 2.56301338,  0.2778626 ],\n",
       "       [ 2.41874618,  0.3047982 ],\n",
       "       [ 1.94410979,  0.1875323 ],\n",
       "       [ 1.52716661, -0.37531698],\n",
       "       [ 1.76434572,  0.07885885],\n",
       "       [ 1.90094161,  0.11662796],\n",
       "       [ 1.39018886, -0.28266094]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "\n",
    "# Perform PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_new = pca.fit_transform(X)\n",
    "X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a8f718",
   "metadata": {},
   "source": [
    "9. Make a few quick notes on:\n",
    "\n",
    "PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb50b7",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique used to reduce the number of features in a dataset by transforming it into a lower-dimensional space while preserving as much of the original variance as possible. It is commonly used in data preprocessing and visualization tasks.\n",
    "\n",
    "Vectors are mathematical entities that have both magnitude and direction. In machine learning, vectors are often used to represent data points in high-dimensional spaces. They can be manipulated using various mathematical operations to perform tasks such as classification and clustering.\n",
    "\n",
    "Embedded techniques refer to feature selection methods that are integrated into the model training process. This means that feature selection is performed automatically as part of the model fitting process, rather than as a separate step in the pipeline. Examples of embedded techniques include Lasso and Ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe439a6",
   "metadata": {},
   "source": [
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c74ae5",
   "metadata": {},
   "source": [
    "Sequential backward exclusion vs. sequential forward selection:\n",
    "Both sequential backward exclusion (SBE) and sequential forward selection (SFS) are feature selection methods that involve selecting and removing features from the dataset. The main difference between the two is the order in which they perform these operations. SBE starts with all the features and removes them one by one, whereas SFS starts with no features and adds them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "313e80ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris#sbe\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "sbe = SequentialFeatureSelector(clf, direction='backward', n_features_to_select=2)\n",
    "sbe.fit(X, y)\n",
    "\n",
    "X_new = sbe.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b639b79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris#sfs\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clf = LogisticRegression()\n",
    "\n",
    "sfs = SequentialFeatureSelector(clf, direction='forward', n_features_to_select=2)\n",
    "sfs.fit(X, y)\n",
    "\n",
    "X_new = sfs.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d94687e",
   "metadata": {},
   "source": [
    "Function selection methods: filter vs. wrapper:\n",
    "Filter and wrapper methods are two types of feature selection methods. Filter methods evaluate the relevance of each feature to the target variable independently of the model, whereas wrapper methods use the model to evaluate the relevance of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb5bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d0cdda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "estimator = LogisticRegression()\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937a0828",
   "metadata": {},
   "source": [
    "SMC vs. Jaccard coefficient:\n",
    "The Jaccard coefficient and SMC are similarity measures used in machine learning. The Jaccard coefficient measures the similarity between two sets by dividing the size of the intersection by the size of the union. SMC measures the similarity between two sets by dividing the size of the intersection by the size of the smaller set.\n",
    "Python code for Jaccard coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30d552b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "set1 = [1, 1, 0, 0, 1, 0, 1, 1]\n",
    "set2 = [1, 0, 0, 1, 1, 0, 0, 1]\n",
    "\n",
    "jaccard_sim = jaccard_score(set1, set2)\n",
    "print(jaccard_sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc104a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  0.4 0.6]\n",
      " [0.4 0.  1. ]\n",
      " [0.6 1.  0. ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# Define the sets\n",
    "set1 = {1, 2, 3, 4, 5}\n",
    "set2 = {1, 3, 5, 7, 9}\n",
    "set3 = {2, 4, 6, 8, 10}\n",
    "\n",
    "# Convert the sets to binary vectors\n",
    "vector1 = [1 if i in set1 else 0 for i in range(1, 11)]\n",
    "vector2 = [1 if i in set2 else 0 for i in range(1, 11)]\n",
    "vector3 = [1 if i in set3 else 0 for i in range(1, 11)]\n",
    "\n",
    "# Create a list of binary vectors\n",
    "vectors = [vector1, vector2, vector3]\n",
    "\n",
    "# Calculate the pairwise distances between the sets\n",
    "distances = pairwise_distances(vectors, metric='hamming')\n",
    "print(distances)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c96bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
