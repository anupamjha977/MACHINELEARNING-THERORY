{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4651238",
   "metadata": {},
   "source": [
    "1.What is prior probability? Give an example.\n",
    "\n",
    "Prior probability is the probability assigned to an event or hypothesis before considering any evidence or data. It represents our belief about the event or hypothesis based on our prior knowledge. \n",
    "\n",
    "An example of prior probability is the probability of getting a head in a coin toss, which is 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05b1331",
   "metadata": {},
   "source": [
    "2.What is posterior probability? Give an example.\n",
    "\n",
    "Posterior probability is the updated probability of an event or hypothesis after considering the evidence or data. It represents our belief about the event or hypothesis based on the new information. \n",
    "\n",
    "An example of posterior probability is the probability of having a disease given a positive test result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ce8c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615374"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of posterior probability\n",
    "prior_prob = 0.01  # Prior probability of having a disease\n",
    "sensitivity = 0.9  # Probability of testing positive given that the person has the disease\n",
    "specificity = 0.95  # Probability of testing negative given that the person does not have the disease\n",
    "test_result = True  # Positive test result\n",
    "# Calculate posterior probability using Bayes' theorem\n",
    "posterior_prob = (sensitivity * prior_prob) / ((sensitivity * prior_prob) + ((1 - specificity) * (1 - prior_prob)))\n",
    "posterior_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8184c68",
   "metadata": {},
   "source": [
    "3.What is likelihood probability? Give an example.\n",
    "\n",
    "Likelihood probability is the probability of observing the evidence or data given a hypothesis or model. It represents how well the hypothesis or model fits the data. \n",
    "\n",
    "An example of likelihood probability is the probability of observing a certain number of heads in a sequence of coin tosses given the probability of getting a head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e33bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24609375000000003"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of likelihood probability\n",
    "p_head = 0.5  # Probability of getting a head\n",
    "num_tosses = 10\n",
    "num_heads = 5  # Number of observed heads\n",
    "# Calculate likelihood probability using binomial distribution\n",
    "from scipy.stats import binom\n",
    "likelihood_prob = binom.pmf(num_heads, num_tosses, p_head)\n",
    "likelihood_prob "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2910d",
   "metadata": {},
   "source": [
    "4.What is Naïve Bayes classifier? Why is it named so?\n",
    "\n",
    "Naïve Bayes classifier is a classification algorithm based on Bayes' theorem that uses the assumption of conditional independence among the features given the class label. It is named \"naïve\" because it assumes that the features are independent, which is not true in most real-world datasets. However, despite this simplifying assumption, Naïve Bayes classifier often performs well in practice and is widely used for text classification and spam filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598917f5",
   "metadata": {},
   "source": [
    "5.What is optimal Bayes classifier?\n",
    "\n",
    "Optimal Bayes classifier is a classification algorithm based on Bayes' theorem that assigns the class label with the highest posterior probability given the evidence or data. It is optimal in the sense that it minimizes the misclassification rate and maximizes the accuracy of the classification. However, in practice, it is often difficult to estimate the prior and likelihood probabilities accurately, which can lead to biased or suboptimal classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e66028",
   "metadata": {},
   "source": [
    "6.Write any two features of Bayesian learning methods.\n",
    "Two features of Bayesian learning methods are:\n",
    "\n",
    "Bayesian learning methods provide a principled framework for incorporating prior knowledge and uncertainty into the learning process, which can improve the accuracy and robustness of the models.\n",
    "Bayesian learning methods can handle complex models with many parameters by using techniques such as Monte Carlo methods and Markov chain Monte Carlo methods to estimate the posterior distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a62a8",
   "metadata": {},
   "source": [
    "7.Define the concept of consistent learners.\n",
    "\n",
    "A consistent learner is a learning algorithm that, given infinite data, will converge to the true underlying function that generates the data. In other words, the algorithm will make fewer and fewer errors as the size of the training data increases, and it will eventually make no errors. A learner is said to be consistent if the difference between its output and the true function becomes arbitrarily small as the sample size increases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c016ded3",
   "metadata": {},
   "source": [
    "8.Write any two strengths of Bayes classifier.\n",
    "\n",
    "The Bayes classifier can handle both discrete and continuous input and output variables.\n",
    "It can effectively handle high-dimensional data with relatively small sample sizes, as it relies on probability distributions rather than point estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2785f4ab",
   "metadata": {},
   "source": [
    "9.Write any two weaknesses of Bayes classifier.\n",
    "\n",
    "The Bayes classifier assumes that the input features are independent, which may not be the case in some real-world scenarios.\n",
    "It can be sensitive to irrelevant features, which may affect the classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e96b7e",
   "metadata": {},
   "source": [
    "10.Explain how Naïve Bayes classifier is used for:\n",
    "\n",
    "Text classification:\n",
    "\n",
    "Naïve Bayes classifier is a popular algorithm for text classification tasks such as sentiment analysis, spam detection, and topic classification. The algorithm takes the text as input and applies a bag-of-words approach to represent the text as a vector of word frequencies or presence/absence indicators. Then, it estimates the likelihood of each class (e.g., positive/negative for sentiment analysis) based on the frequencies of words in the training data. Finally, it applies Bayes' theorem to calculate the posterior probabilities of each class given the input text and chooses the class with the highest probability as the output.\n",
    "\n",
    "Spam filtering:\n",
    "Naïve Bayes classifier is a popular algorithm for spam filtering because it can effectively handle high-dimensional data with relatively small sample sizes. The algorithm takes the email message as input and extracts a set of features such as the sender's address, the message header, and the content. Then, it applies Bayes' theorem to estimate the posterior probability of the message being spam or not based on the frequencies of these features in the training data. Finally, it compares the posterior probabilities of the two classes and classifies the message as spam or not based on a predefined threshold.\n",
    "\n",
    "Market sentiment analysis:\n",
    "Naïve Bayes classifier can also be used for market sentiment analysis, which aims to predict the direction of the stock market based on the sentiment of news articles, social media posts, and other sources. The algorithm takes the text as input and applies a bag-of-words approach to represent the sentiment of each article as a vector of word frequencies or presence/absence indicators. Then, it estimates the likelihood of each class (e.g., positive/negative for market sentiment) based on the frequencies of words in the training data. Finally, it applies Bayes' theorem to calculate the posterior probabilities of each class given the input text and chooses the class with the highest probability as the output.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797d491",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
