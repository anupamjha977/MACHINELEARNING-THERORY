{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd1c43fe",
   "metadata": {},
   "source": [
    "1 What exactly is a feature? Give an example to illustrate your point.\n",
    "\n",
    "A: In machine learning, a feature refers to a measurable aspect or property of a phenomenon being observed. For example, in a dataset of houses and their sale prices, features could include the number of bedrooms, the square footage of the house, the year it was built, etc. These features are used as inputs to a machine learning algorithm to make predictions or classifications.\n",
    "\n",
    "Here's an example Python code snippet that defines a simple dataset of houses and their "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "046893fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_bedrooms  sq_footage  year_built  sale_price\n",
      "0             2        1000        1970      150000\n",
      "1             3        1500        1990      250000\n",
      "2             4        2000        2005      350000\n",
      "3             2        1200        1985      200000\n",
      "4             3        1800        2010      300000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define a dictionary of features for each house\n",
    "houses = {\n",
    "    'num_bedrooms': [2, 3, 4, 2, 3],\n",
    "    'sq_footage': [1000, 1500, 2000, 1200, 1800],\n",
    "    'year_built': [1970, 1990, 2005, 1985, 2010],\n",
    "    'sale_price': [150000, 250000, 350000, 200000, 300000]\n",
    "}\n",
    "\n",
    "# create a Pandas DataFrame from the dictionary\n",
    "df = pd.DataFrame(houses)\n",
    "\n",
    "# display the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f5b159",
   "metadata": {},
   "source": [
    "2.What are the various circumstances in which feature construction is required?\n",
    "Answer:\n",
    "Feature construction, also known as feature engineering, is the process of creating new features from the existing ones to improve the performance of machine learning models. It is required in the following circumstances:\n",
    "\n",
    "When the existing features are insufficient: Sometimes, the existing features in a dataset may not be sufficient to solve a problem. In such cases, new features need to be created to help the machine learning model learn the underlying patterns in the data.\n",
    "\n",
    "To improve model accuracy: Feature engineering can help improve the accuracy of machine learning models. By creating new features that are more relevant to the problem at hand, the model can learn more efficiently and make better predictions.\n",
    "\n",
    "To reduce overfitting: Overfitting occurs when a model becomes too complex and learns the training data too well, leading to poor performance on new data. Feature engineering can help prevent overfitting by creating new features that capture more general patterns in the data.\n",
    "\n",
    "To improve interpretability: Feature engineering can help improve the interpretability of machine learning models. By creating features that are more interpretable, we can gain a better understanding of how the model is making predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d786fcca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (150, 4)\n",
      "Transformed shape: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# extract the features and target variable\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# perform PCA to construct new features\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# print the shape of the original and transformed feature matrices\n",
    "print(\"Original shape:\", X.shape)\n",
    "print(\"Transformed shape:\", X_pca.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69818f",
   "metadata": {},
   "source": [
    "3.Describe how nominal variables are encoded.\n",
    "Nominal variables are categorical variables that do not have an inherent order or ranking. One common method for encoding nominal variables is one-hot encoding, where a binary variable is created for each category in the nominal variable. This method results in a sparse matrix where most values are 0, except for the binary variable corresponding to the category that is present in the particular instance.\n",
    "\n",
    "For example, consider a nominal variable \"color\" with categories \"red\", \"blue\", and \"green\". One-hot encoding would create three binary variables \"color_red\", \"color_blue\", and \"color_green\". If an instance has a value of \"red\" for the \"color\" variable, the corresponding binary variable \"color_red\" would be set to 1, and the other two binary variables would be 0.\n",
    "\n",
    "Python code to encode nominal variables using one-hot encoding in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a44ac465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "# create example nominal data\n",
    "nominal_data = np.array([[\"red\"], [\"blue\"], [\"green\"], [\"red\"], [\"green\"]])\n",
    "\n",
    "# create one-hot encoder\n",
    "encoder = OneHotEncoder()\n",
    "encoded_data = encoder.fit_transform(nominal_data)\n",
    "\n",
    "# print encoded data\n",
    "print(encoded_data.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a6dcf",
   "metadata": {},
   "source": [
    "4.Describe how numeric features are converted to categorical features.\n",
    "Answer:\n",
    "Numeric features can be converted to categorical features by binning or discretization. Binning involves dividing the numeric values into discrete intervals or bins, and then assigning each value to the corresponding bin. Discretization involves creating a set of thresholds and then mapping each value to the appropriate threshold, effectively converting the continuous numeric variable into a set of categories.\n",
    "\n",
    "Python code to convert numeric features to categorical features using binning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d5a0664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD    TAX  \\\n",
      "0  0.00632  18.0   2.31   0.0  0.538  6.575   65.2  4.0900  1.0  296.0   \n",
      "1  0.02731   0.0   7.07   0.0  0.469  6.421   78.9  4.9671  2.0  242.0   \n",
      "2  0.02729   0.0   7.07   0.0  0.469  7.185   61.1  4.9671  2.0  242.0   \n",
      "3  0.03237   0.0   2.18   0.0  0.458  6.998   45.8  6.0622  3.0  222.0   \n",
      "4  0.06905   0.0   2.18   0.0  0.458  7.147   54.2  6.0622  3.0  222.0   \n",
      "5  0.02985   0.0   2.18   0.0  0.458  6.430   58.7  6.0622  3.0  222.0   \n",
      "6  0.08829  12.5   7.87   0.0  0.524  6.012   66.6  5.5605  5.0  311.0   \n",
      "7  0.14455  12.5   7.87   0.0  0.524  6.172   96.1  5.9505  5.0  311.0   \n",
      "8  0.21124  12.5   7.87   0.0  0.524  5.631  100.0  6.0821  5.0  311.0   \n",
      "9  0.17004  12.5   7.87   0.0  0.524  6.004   85.9  6.5921  5.0  311.0   \n",
      "\n",
      "   PTRATIO       B  LSTAT      AGE_binned  \n",
      "0     15.3  396.90   4.98  (61.16, 80.58]  \n",
      "1     17.8  396.90   9.14  (61.16, 80.58]  \n",
      "2     17.8  392.83   4.03  (41.74, 61.16]  \n",
      "3     18.7  394.63   2.94  (41.74, 61.16]  \n",
      "4     18.7  396.90   5.33  (41.74, 61.16]  \n",
      "5     18.7  394.12   5.21  (41.74, 61.16]  \n",
      "6     15.2  395.60  12.43  (61.16, 80.58]  \n",
      "7     15.2  396.90  19.15  (80.58, 100.0]  \n",
      "8     15.2  386.63  29.93  (80.58, 100.0]  \n",
      "9     15.2  386.71  17.10  (80.58, 100.0]  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "\n",
    "# load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "\n",
    "# create a pandas dataframe from the dataset\n",
    "df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "# bin the 'AGE' feature into 5 bins\n",
    "df['AGE_binned'] = pd.cut(df['AGE'], bins=5)\n",
    "\n",
    "# display the first 10 rows of the dataframe\n",
    "print(df.head(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2069644c",
   "metadata": {},
   "source": [
    "5.Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?\n",
    "\n",
    "Answer:\n",
    "\n",
    "The feature selection wrapper approach involves using a model to select the best set of features for a given task by repeatedly training and evaluating the model on different subsets of features. The general procedure involves the following steps:\n",
    "\n",
    "Select an initial set of features\n",
    "Train a model using only these features\n",
    "Evaluate the performance of the model using a validation set\n",
    "Select a subset of the features (using some criterion, e.g. the highest importance scores) and repeat from step 2 until some stopping criterion is met.\n",
    "Advantages:\n",
    "\n",
    "It takes into account the interaction between features, which can result in more accurate models.\n",
    "It can improve the efficiency of models by reducing the number of features used.\n",
    "Disadvantages:\n",
    "\n",
    "It can be computationally expensive, especially for large datasets or models.\n",
    "It can be prone to overfitting if the stopping criterion is not well defined or if the model is too complex.\n",
    "Python code example using the wrapper method for feature selection with Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "296202d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False False False False False False False False  True False\n",
      " False  True False False False False False False  True  True False False\n",
      " False False  True False  True False False False False False False False\n",
      "  True False False False False False  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False  True False False]\n",
      "[55 38  6 30 28 15 31 43 49 29  1 35 23  1 36 42 50 33 16  4  1  1 34 44\n",
      " 47 27  1  9  1 12  2 51 54  5 11 24  1 20  7 53 45 32  1  1 13 19 18 46\n",
      " 48 39 21 10 25 17  8 40 52 41 14 26  3  1 22 37]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# define the classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# use RFE to select the best features\n",
    "selector = RFE(clf, n_features_to_select=10, step=1)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# print the best features\n",
    "print(selector.support_)\n",
    "print(selector.ranking_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde42cc9",
   "metadata": {},
   "source": [
    "6.When is a feature considered irrelevant? What can be said to quantify it?\n",
    "Answer:\n",
    "\n",
    "A feature is considered irrelevant when it does not contribute to the target variable or has no significant impact on the accuracy of the predictive model. Irrelevant features add unnecessary complexity to the model, which can lead to overfitting and reduced model performance.\n",
    "\n",
    "The feature importance score is used to quantify the relevance of each feature. This score indicates the extent to which a feature contributes to the model's accuracy. Higher importance scores suggest that the feature is more relevant and contributes more to the model's predictive power.\n",
    "\n",
    "Python Code:\n",
    "\n",
    "Here's an example of how to calculate feature importance scores using the random forest classifier in scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b081a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 0: importance score = 0.0598\n",
      "Feature 1: importance score = 0.0125\n",
      "Feature 2: importance score = 0.0552\n",
      "Feature 3: importance score = 0.0401\n",
      "Feature 4: importance score = 0.0059\n",
      "Feature 5: importance score = 0.0152\n",
      "Feature 6: importance score = 0.0452\n",
      "Feature 7: importance score = 0.0964\n",
      "Feature 8: importance score = 0.0035\n",
      "Feature 9: importance score = 0.0058\n",
      "Feature 10: importance score = 0.0175\n",
      "Feature 11: importance score = 0.0059\n",
      "Feature 12: importance score = 0.0050\n",
      "Feature 13: importance score = 0.0341\n",
      "Feature 14: importance score = 0.0038\n",
      "Feature 15: importance score = 0.0033\n",
      "Feature 16: importance score = 0.0087\n",
      "Feature 17: importance score = 0.0042\n",
      "Feature 18: importance score = 0.0038\n",
      "Feature 19: importance score = 0.0062\n",
      "Feature 20: importance score = 0.1006\n",
      "Feature 21: importance score = 0.0193\n",
      "Feature 22: importance score = 0.1008\n",
      "Feature 23: importance score = 0.1049\n",
      "Feature 24: importance score = 0.0125\n",
      "Feature 25: importance score = 0.0099\n",
      "Feature 26: importance score = 0.0433\n",
      "Feature 27: importance score = 0.1599\n",
      "Feature 28: importance score = 0.0106\n",
      "Feature 29: importance score = 0.0060\n",
      "Important features: [20, 22, 23, 27]\n",
      "Unimportant features: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 24, 25, 26, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Train the random forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Print the feature importances\n",
    "importances = clf.feature_importances_\n",
    "for i, importance in enumerate(importances):\n",
    "    print(f\"Feature {i}: importance score = {importance:.4f}\")\n",
    "\n",
    "# Identify the most important features\n",
    "threshold = 0.1  # Example threshold value\n",
    "important_indices = [i for i, importance in enumerate(importances) if importance >= threshold]\n",
    "print(\"Important features:\", important_indices)\n",
    "print(\"Unimportant features:\", [i for i in range(X.shape[1]) if i not in important_indices])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fdbe7f",
   "metadata": {},
   "source": [
    "8.When is a function considered redundant? What criteria are used to identify features that could be redundant?\n",
    "Answer:\n",
    "A feature is considered redundant if it provides the same information as another feature or set of features. A function is considered redundant if it can be expressed as a linear combination of other functions. In general, the criteria used to identify redundant features are correlation and mutual information. Features with high correlation or mutual information are likely to be redundant.\n",
    "\n",
    "Correlation can be measured using Pearson's correlation coefficient or Spearman's rank correlation coefficient. Pearson's correlation coefficient measures the linear relationship between two variables, while Spearman's rank correlation coefficient measures the monotonic relationship between two variables. In both cases, a coefficient close to 1 indicates a strong positive correlation, while a coefficient close to -1 indicates a strong negative correlation.\n",
    "\n",
    "Mutual information is a measure of the amount of information shared by two variables. It is commonly used in feature selection to measure the relevance of a feature to a target variable. Features with high mutual information are likely to be informative and useful, while features with low mutual information are likely to be redundant.\n",
    "\n",
    "Python code to identify redundant features using correlation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8527bc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:\n",
      "1. mean radius\n",
      "2. mean perimeter\n",
      "3. mean area\n",
      "4. mean concavity\n",
      "5. mean concave points\n",
      "6. worst radius\n",
      "7. worst perimeter\n",
      "8. worst area\n",
      "9. worst concavity\n",
      "10. worst concave points\n",
      "Ignored features:\n",
      "1. mean texture\n",
      "2. mean smoothness\n",
      "3. mean compactness\n",
      "4. mean symmetry\n",
      "5. mean fractal dimension\n",
      "6. radius error\n",
      "7. texture error\n",
      "8. perimeter error\n",
      "9. area error\n",
      "10. smoothness error\n",
      "11. compactness error\n",
      "12. concavity error\n",
      "13. concave points error\n",
      "14. symmetry error\n",
      "15. fractal dimension error\n",
      "16. worst texture\n",
      "17. worst smoothness\n",
      "18. worst compactness\n",
      "19. worst symmetry\n",
      "20. worst fractal dimension\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# load breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# apply feature selection using ANOVA F-value as score function\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "selector.fit(X, y)\n",
    "\n",
    "# print selected features and their scores\n",
    "print(\"Selected features:\")\n",
    "for i, feature in enumerate(data.feature_names[selector.get_support()]):\n",
    "    print(f\"{i+1}. {feature}\")\n",
    "    \n",
    "print(\"Ignored features:\")\n",
    "for i, feature in enumerate(data.feature_names[~selector.get_support()]):\n",
    "    print(f\"{i+1}. {feature}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e5d58",
   "metadata": {},
   "source": [
    "This code loads the breast cancer dataset from scikit-learn, applies feature selection using ANOVA F-value as the score function, and prints the names of the selected and ignored features. The SelectKBest function is used to select the top k features based on the score function, which in this case is f_classif for ANOVA F-value. The get_support method is used to obtain a Boolean mask of the selected features, and the ~ operator is used to obtain the mask of ignored features. The feature names are obtained from the feature_names attribute of the dataset object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca37d2d1",
   "metadata": {},
   "source": [
    "9.State difference between Euclidean and Manhattan distances?\n",
    "Euclidean distance and Manhattan distance are two common distance metrics used in machine learning and data analysis. The main difference between them is the way they measure distance between two points in a feature space.\n",
    "\n",
    "Euclidean distance: It is the straight-line distance between two points in a feature space. It is calculated as the square root of the sum of the squared differences between the corresponding elements of the two points. It is the most commonly used distance metric and is suitable for continuous variables.\n",
    "\n",
    "Manhattan distance: It is also called the taxicab distance or L1 norm distance. It is the sum of the absolute differences between the corresponding elements of the two points. It is suitable for both continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06ff38a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance: 5.0\n",
      "Manhattan distance: 7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define two points in a 2D feature space\n",
    "point1 = np.array([1, 2])\n",
    "point2 = np.array([4, 6])\n",
    "\n",
    "# calculate Euclidean distance\n",
    "euclidean_dist = np.linalg.norm(point1 - point2)\n",
    "print(\"Euclidean distance:\", euclidean_dist)\n",
    "\n",
    "# calculate Manhattan distance\n",
    "manhattan_dist = np.sum(np.abs(point1 - point2))\n",
    "print(\"Manhattan distance:\", manhattan_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc465ddf",
   "metadata": {},
   "source": [
    "10.Distinguish between feature transformation and feature selection.\n",
    "Answer:\n",
    "\n",
    "Feature transformation and feature selection are two important techniques used in machine learning to preprocess and prepare data for modeling.\n",
    "\n",
    "Feature transformation involves changing the scale or distribution of the feature data. This can be done in several ways, such as scaling the data to a common range, normalizing the data to have zero mean and unit variance, or performing logarithmic or power transformations to change the data distribution. Feature transformation aims to improve the performance of machine learning algorithms by making the data more suitable for modeling.\n",
    "\n",
    "On the other hand, feature selection involves selecting a subset of the original features to be used in the modeling process. This can be done using a variety of techniques, such as statistical tests, correlation analysis, or machine learning algorithms themselves. Feature selection aims to improve the performance of machine learning algorithms by reducing the dimensionality of the data and removing irrelevant or redundant features.\n",
    "\n",
    "Python code:\n",
    "\n",
    "Here's an example of feature transformation and feature selection using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2bf776b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# apply feature transformation by scaling the data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# apply feature selection by selecting the top 2 features based on ANOVA F-test\n",
    "selector = SelectKBest(f_classif, k=2)\n",
    "X_train = selector.fit_transform(X_train, y_train)\n",
    "X_test = selector.transform(X_test)\n",
    "\n",
    "# train a logistic regression model on the selected features\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# evaluate the model on the testing set\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bc15e",
   "metadata": {},
   "source": [
    "in this example, the iris dataset is first split into training and testing sets. Then, feature transformation is applied by scaling the data using the StandardScaler transformer. Feature selection is then applied by selecting the top 2 features based on ANOVA F-test using the SelectKBest selector. Finally, a logistic regression model is trained on the selected features and evaluated on the testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7f7439",
   "metadata": {},
   "source": [
    "11. Make brief notes on any two of the following:\n",
    "\n",
    "          1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "          2. Collection of features using a hybrid approach\n",
    "\n",
    "          3. The width of the silhouette\n",
    "\n",
    "          4. Receiver operating characteristic curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75ac914",
   "metadata": {},
   "source": [
    "SVD (Standard Variable Diameter Diameter):\n",
    "SVD is a dimensionality reduction technique that is used to find patterns in high-dimensional data. It is based on the Singular Value Decomposition (SVD) of a matrix, which allows the matrix to be represented in terms of its singular values and singular vectors. SVD is particularly useful when dealing with large datasets that have many features, as it can reduce the number of features while preserving the underlying structure of the data.\n",
    "\n",
    "Collection of features using a hybrid approach:\n",
    "A hybrid approach to feature selection combines both filter and wrapper methods. In this approach, a subset of features is first selected using a filter method, and then a wrapper method is used to further refine the subset of features. The filter method is used to eliminate irrelevant features based on statistical tests, while the wrapper method is used to evaluate subsets of features using a machine learning algorithm.\n",
    "\n",
    "Here's an example of how to use the hybrid approach for feature selection in Python using scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c4d735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features: ['petal length (cm)' 'petal width (cm)']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Create a pipeline with a feature selection step and a logistic regression step\n",
    "pipeline = Pipeline([\n",
    "  ('feature_selection', SelectKBest(f_classif, k=2)),\n",
    "  ('classification', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the selected features\n",
    "selected_indices = np.where(pipeline.named_steps['feature_selection'].get_support())[0]\n",
    "selected_features = np.array(iris.feature_names)[selected_indices]\n",
    "print('Selected features:', selected_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f0d4f",
   "metadata": {},
   "source": [
    "The width of the silhouette: The silhouette width is a metric used to measure how similar an object is to its own cluster (cohesion) compared to other clusters (separation). It takes values between -1 and 1, where higher values indicate better cluster assignments. A score of 1 indicates a very good clustering, while a score of -1 indicates that the object is probably assigned to the wrong cluster. The width of the silhouette can be calculated using the silhouette_score function from the sklearn.metrics module.\n",
    "\n",
    "Receiver Operating Characteristic Curve (ROC Curve): A ROC curve is a graphical representation of the performance of a binary classification model. It is created by plotting the true positive rate (sensitivity) against the false positive rate (1-specificity) at various thresholds. The area under the ROC curve (AUC) is a measure of the model's accuracy, with values closer to 1 indicating better performance. The ROC curve can be generated using the roc_curve function from the sklearn.metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73019e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.16 in c:\\users\\lenovo\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette score: 0.5528190123564095\n",
      "AUC score: 0.4375\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# calculate the silhouette score for a k-means clustering with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "labels = kmeans.fit_predict(X)\n",
    "silhouette_avg = silhouette_score(X, labels)\n",
    "print(f\"Silhouette score: {silhouette_avg}\")\n",
    "\n",
    "# plot the ROC curve for a logistic regression model\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "y_prob = lr.predict_proba(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1], pos_label=2)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(f\"AUC score: {roc_auc}\")\n",
    "\n",
    "plt.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], '--', color='gray', label='Random guessing')\n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic (ROC) curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29067efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
